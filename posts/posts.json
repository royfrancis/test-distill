[
  {
    "path": "posts/2021-06-29-the-sharpe-ratio/",
    "title": "The Sharpe Ratio",
    "description": "A description of the Sharpe ratio.",
    "author": [
      {
        "name": "rmf",
        "url": {}
      }
    ],
    "date": "2021-06-29",
    "categories": [
      "Web"
    ],
    "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-06-29-the-sharpe-ratio/featured.jpg",
    "last_modified": "2021-06-29T19:29:07+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-09-06-keras-for-r/",
    "title": "Keras for R",
    "description": "We are excited to announce that the keras package is now available on CRAN. The package provides an R interface to Keras, a high-level neural networks API developed with a focus on enabling fast experimentation.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-09-05",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases"
    ],
    "contents": "\nWe are excited to announce that the keras package is now available on CRAN. The package provides an R interface to Keras, a high-level neural networks API developed with a focus on enabling fast experimentation. Keras has the following key features:\nAllows the same code to run on CPU or on GPU, seamlessly.\nUser-friendly API which makes it easy to quickly prototype deep learning models.\nBuilt-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.\nSupports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc. This means that Keras is appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.\nIs capable of running on top of multiple back-ends including TensorFlow, CNTK, or Theano.\nIf you are already familiar with Keras and want to jump right in, check out https://tensorflow.rstudio.com/keras which has everything you need to get started including over 20 complete examples to learn from.\nTo learn a bit more about Keras and why we’re so excited to announce the Keras interface for R, read on!\nKeras and Deep Learning\nInterest in deep learning has been accelerating rapidly over the past few years, and several deep learning frameworks have emerged over the same time frame. Of all the available frameworks, Keras has stood out for its productivity, flexibility and user-friendly API. At the same time, TensorFlow has emerged as a next-generation machine learning platform that is both extremely flexible and well-suited to production deployment.\nNot surprisingly, Keras and TensorFlow have of late been pulling away from other deep learning frameworks:\n\n\nGoogle web search interest around deep learning frameworks over time. If you remember Q4 2015 and Q1-2 2016 as confusing, you weren't alone. pic.twitter.com/1f1VQVGr8n\n\n— François Chollet (@fchollet) June 3, 2017\n\nThe good news about Keras and TensorFlow is that you don’t need to choose between them! The default backend for Keras is TensorFlow and Keras can be integrated seamlessly with TensorFlow workflows. There is also a pure-TensorFlow implementation of Keras with deeper integration on the roadmap for later this year.\nKeras and TensorFlow are the state of the art in deep learning tools and with the keras package you can now access both with a fluent R interface.\nGetting Started\nInstallation\nTo begin, install the keras R package from CRAN as follows:\n\n\ninstall.packages(\"keras\")\n\nThe Keras R interface uses the TensorFlow backend engine by default. To install both the core Keras library as well as the TensorFlow backend use the install_keras() function:\n\n\nlibrary(keras)\ninstall_keras()\n\nThis will provide you with default CPU-based installations of Keras and TensorFlow. If you want a more customized installation, e.g. if you want to take advantage of NVIDIA GPUs, see the documentation for install_keras().\nMNIST Example\nWe can learn the basics of Keras by walking through a simple example: recognizing handwritten digits from the MNIST dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits like these:\n\nThe dataset also includes labels for each image, telling us which digit it is. For example, the labels for the above images are 5, 0, 4, and 1.\nPreparing the Data\nThe MNIST dataset is included with Keras and can be accessed using the dataset_mnist() function. Here we load the dataset then create variables for our test and training data:\n\n\nlibrary(keras)\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\nThe x data is a 3-d array (images,width,height) of grayscale values. To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors). Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1:\n\n\n# reshape\ndim(x_train) <- c(nrow(x_train), 784)\ndim(x_test) <- c(nrow(x_test), 784)\n# rescale\nx_train <- x_train / 255\nx_test <- x_test / 255\n\nThe y data is an integer vector with values ranging from 0 to 9. To prepare this data for training we one-hot encode the vectors into binary class matrices using the Keras to_categorical() function:\n\n\ny_train <- to_categorical(y_train, 10)\ny_test <- to_categorical(y_test, 10)\n\nDefining the Model\nThe core data structure of Keras is a model, a way to organize layers. The simplest type of model is the sequential model, a linear stack of layers.\nWe begin by creating a sequential model and then adding layers using the pipe (%>%) operator:\n\n\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 256, activation = \"relu\", input_shape = c(784)) %>% \n  layer_dropout(rate = 0.4) %>% \n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\nThe input_shape argument to the first layer specifies the shape of the input data (a length 784 numeric vector representing a grayscale image). The final layer outputs a length 10 numeric vector (probabilities for each digit) using a softmax activation function.\nUse the summary() function to print the details of the model:\n\n\nsummary(model)\n\nModel\n________________________________________________________________________________\nLayer (type)                        Output Shape                    Param #     \n================================================================================\ndense_1 (Dense)                     (None, 256)                     200960      \n________________________________________________________________________________\ndropout_1 (Dropout)                 (None, 256)                     0           \n________________________________________________________________________________\ndense_2 (Dense)                     (None, 128)                     32896       \n________________________________________________________________________________\ndropout_2 (Dropout)                 (None, 128)                     0           \n________________________________________________________________________________\ndense_3 (Dense)                     (None, 10)                      1290        \n================================================================================\nTotal params: 235,146\nTrainable params: 235,146\nNon-trainable params: 0\n________________________________________________________________________________\nNext, compile the model with appropriate loss function, optimizer, and metrics:\n\n\nmodel %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = optimizer_rmsprop(),\n  metrics = c(\"accuracy\")\n)\n\nTraining and Evaluation\nUse the fit() function to train the model for 30 epochs using batches of 128 images:\n\n\nhistory <- model %>% fit(\n  x_train, y_train, \n  epochs = 30, batch_size = 128, \n  validation_split = 0.2\n)\n\nThe history object returned by fit() includes loss and accuracy metrics which we can plot:\n\n\nplot(history)\n\n\nEvaluate the model’s performance on the test data:\n\n\nmodel %>% evaluate(x_test, y_test,verbose = 0)\n\n\n$loss\n[1] 0.1149\n\n$acc\n[1] 0.9807\nGenerate predictions on new data:\n\n\nmodel %>% predict_classes(x_test)\n\n\n  [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7 1 2\n [40] 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9 1 7 3 2\n [79] 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9\n [ reached getOption(\"max.print\") -- omitted 9900 entries ]\nKeras provides a vocabulary for building deep learning models that is simple, elegant, and intuitive. Building a question answering system, an image classification model, a neural Turing machine, or any other model is just as straightforward.\nThe Guide to the Sequential Model article describes the basics of Keras sequential models in more depth.\nExamples\nOver 20 complete examples are available (special thanks to [@dfalbel](https://github.com/dfalbel) for his work on these!). The examples cover image classification, text generation with stacked LSTMs, question-answering with memory networks, transfer learning, variational encoding, and more.\nExample\nDescription\naddition_rnn\nImplementation of sequence to sequence learning for performing addition of two numbers (as strings).\nbabi_memnn\nTrains a memory network on the bAbI dataset for reading comprehension.\nbabi_rnn\nTrains a two-branch recurrent network on the bAbI dataset for reading comprehension.\ncifar10_cnn\nTrains a simple deep CNN on the CIFAR10 small images dataset.\nconv_lstm\nDemonstrates the use of a convolutional LSTM network.\ndeep_dream\nDeep Dreams in Keras.\nimdb_bidirectional_lstm\nTrains a Bidirectional LSTM on the IMDB sentiment classification task.\nimdb_cnn\nDemonstrates the use of Convolution1D for text classification.\nimdb_cnn_lstm\nTrains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task.\nimdb_fasttext\nTrains a FastText model on the IMDB sentiment classification task.\nimdb_lstm\nTrains a LSTM on the IMDB sentiment classification task.\nlstm_text_generation\nGenerates text from Nietzsche’s writings.\nmnist_acgan\nImplementation of AC-GAN (Auxiliary Classifier GAN ) on the MNIST dataset\nmnist_antirectifier\nDemonstrates how to write custom layers for Keras\nmnist_cnn\nTrains a simple convnet on the MNIST dataset.\nmnist_irnn\nReproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units” by Le et al.\nmnist_mlp\nTrains a simple deep multi-layer perceptron on the MNIST dataset.\nmnist_hierarchical_rnn\nTrains a Hierarchical RNN (HRNN) to classify MNIST digits.\nmnist_transfer_cnn\nTransfer learning toy example.\nneural_style_transfer\nNeural style transfer (generating an image with the same “content” as a base image, but with the “style” of a different picture).\nreuters_mlp\nTrains and evaluates a simple MLP on the Reuters newswire topic classification task.\nstateful_lstm\nDemonstrates how to use stateful RNNs to model long sequences efficiently.\nvariational_autoencoder\nDemonstrates how to build a variational autoencoder.\nvariational_autoencoder_deconv\nDemonstrates how to build a variational autoencoder with Keras using deconvolution layers.\nLearning More\nAfter you’ve become familiar with the basics, these articles are a good next step:\nGuide to the Sequential Model. The sequential model is a linear stack of layers and is the API most users should start with.\nGuide to the Functional API. The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\nTraining Visualization. There are a wide variety of tools available for visualizing training. These include plotting of training metrics, real time display of metrics within the RStudio IDE, and integration with the TensorBoard visualization tool included with TensorFlow.\nUsing Pre-Trained Models. Keras includes a number of deep learning models (Xception, VGG16, VGG19, ResNet50, InceptionVV3, and MobileNet) that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\nFrequently Asked Questions. Covers many additional topics including streaming training data, saving models, training on GPUs, and more.\nKeras provides a productive, highly flexible framework for developing deep learning models. We can’t wait to see what the R community will do with these tools!\n\n\n",
    "preview": "posts/2017-09-06-keras-for-r/preview.png",
    "last_modified": "2021-06-29T19:29:07+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-08-31-tensorflow-estimators-for-r/",
    "title": "TensorFlow Estimators",
    "description": "The tfestimators package is an R interface to TensorFlow Estimators, a high-level API that provides implementations of many different model types including linear models and deep neural networks.",
    "author": [
      {
        "name": "Yuan Tang",
        "url": "https://orcid.org/0000-0001-5243-233X"
      }
    ],
    "date": "2017-08-31",
    "categories": [
      "Packages/Releases"
    ],
    "contents": "\nThe tfestimators package is an R interface to TensorFlow Estimators, a high-level API that provides implementations of many different model types including linear models and deep neural networks.\nMore models are coming soon such as state saving recurrent neural networks, dynamic recurrent neural networks, support vector machines, random forest, KMeans clustering, etc. TensorFlow estimators also provides a flexible framework for defining arbitrary new model types as custom estimators.\nThe framework balances the competing demands for flexibility and simplicity by offering APIs at different levels of abstraction, making common model architectures available out of the box, while providing a library of utilities designed to speed up experimentation with model architectures.\nThese abstractions guide developers to write models in ways conducive to productionization as well as making it possible to write downstream infrastructure for distributed training or parameter tuning independent of the model implementation.\nTo make out of the box models flexible and usable across a wide range of problems, tfestimators provides canned Estimators that are are parameterized not only over traditional hyperparameters, but also using feature columns, a declarative specification describing how to interpret input data.\nFor more details on the architecture and design of TensorFlow Estimators, please check out the KDD’17 paper: TensorFlow Estimators: Managing Simplicity vs. Flexibility in High-Level Machine Learning Frameworks.\nQuick Start\nInstallation\nTo use tfestimators, you need to install both the tfestimators R package as well as TensorFlow itself.\nFirst, install the tfestimators R package as follows:\n\n\ndevtools::install_github(\"rstudio/tfestimators\")\n\nThen, use the install_tensorflow() function to install TensorFlow (note that the current tfestimators package requires version 1.3.0 of TensorFlow so even if you already have TensorFlow installed you should update if you are running a previous version):\n\n\nlibrary(tfestimators)\ninstall_tensorflow()\n\nThis will provide you with a default installation of TensorFlow suitable for getting started. See the article on installation to learn about more advanced options, including installing a version of TensorFlow that takes advantage of NVIDIA GPUs if you have the correct CUDA libraries installed.\nLinear Regression\nLet’s create a simple linear regression model with the mtcars dataset to demonstrate the use of estimators. We’ll illustrate how input functions can be constructed and used to feed data to an estimator, how feature columns can be used to specify a set of transformations to apply to input data, and how these pieces come together in the Estimator interface.\nInput Function\nEstimators can receive data through input functions. Input functions take an arbitrary data source (in-memory data sets, streaming data, custom data format, and so on) and generate Tensors that can be supplied to TensorFlow models. The tfestimators package includes an input_fn() function that can create TensorFlow input functions from common R data sources (e.g. data frames and matrices). It’s also possible to write a fully custom input function.\nHere, we define a helper function that will return an input function for a subset of our mtcars data set.\n\n\nlibrary(tfestimators)\n\n# return an input_fn for a given subset of data\nmtcars_input_fn <- function(data) {\n  input_fn(data, \n           features = c(\"disp\", \"cyl\"), \n           response = \"mpg\")\n}\n\nFeature Columns\nNext, we define the feature columns for our model. Feature columns are used to specify how Tensors received from the input function should be combined and transformed before entering the model training, evaluation, and prediction steps. A feature column can be a plain mapping to some input column (e.g. column_numeric() for a column of numerical data), or a transformation of other feature columns (e.g. column_crossed() to define a new column as the cross of two other feature columns).\nHere, we create a list of feature columns containing two numeric variables - disp and cyl:\n\n\ncols <- feature_columns(\n  column_numeric(\"disp\"),\n  column_numeric(\"cyl\")\n)\n\nYou can also define multiple feature columns at once:\n\n\ncols <- feature_columns( \n  column_numeric(\"disp\", \"cyl\")\n)\n\nBy using the family of feature column functions we can define various transformations on the data before using it for modeling.\nEstimator\nNext, we create the estimator by calling the linear_regressor() function and passing it a set of feature columns:\n\n\nmodel <- linear_regressor(feature_columns = cols)\n\nTraining\nWe’re now ready to train our model, using the train() function. We’ll partition the mtcars data set into separate training and validation data sets, and feed the training data set into train(). We’ll hold 20% of the data aside for validation.\n\n\nindices <- sample(1:nrow(mtcars), size = 0.80 * nrow(mtcars))\ntrain <- mtcars[indices, ]\ntest  <- mtcars[-indices, ]\n\n# train the model\nmodel %>% train(mtcars_input_fn(train))\n\nEvaluation\nWe can evaluate the model’s accuracy using the evaluate() function, using our ‘test’ data set for validation.\n\n\nmodel %>% evaluate(mtcars_input_fn(test))\n\nPrediction\nAfter we’ve finished training out model, we can use it to generate predictions from new data.\n\n\nnew_obs <- mtcars[1:3, ]\nmodel %>% predict(mtcars_input_fn(new_obs))\n\nLearning More\nAfter you’ve become familiar with these concepts, these articles cover the basics of using TensorFlow Estimators and the main components in more detail:\nEstimator Basics\nInput Functions\nFeature Columns\nThese articles describe more advanced topics/usage:\nRun Hooks\nCustom Estimators\nTensorFlow Layers\nTensorBoard Visualization\nParsing Utilities\nOne of the best ways to learn is from reviewing and experimenting with examples. See the Examples page for a variety of examples to help you get started.\n\n\n",
    "preview": "posts/2017-08-31-tensorflow-estimators-for-r/tensorflow-architecture.png",
    "last_modified": "2021-06-29T19:29:07+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-08-17-tensorflow-v13-released/",
    "title": "TensorFlow v1.3 Released",
    "description": "The final release of TensorFlow v1.3 is now available. This release marks the initial availability of several canned estimators including DNNClassifier and  DNNRegressor.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-08-17",
    "categories": [
      "Packages/Releases"
    ],
    "contents": "\nThe final release of TensorFlow v1.3 is now available. This release of TensorFlow marks the initial availability of several canned estimators, including:\nDNNClassifier\nDNNRegressor\nLinearClassifier\nLinearRegressor\nDNNLinearCombinedClassifier\nDNNLinearCombinedRegressor.\nThe tfestimators package provides a high level R interface for these estimators.\nFull details on the release of TensorFlow v1.3 are available here: https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0\nYou can update your R installation of TensorFlow using the install_tensorflow function:\n\nlibrary(tensorflow)\ninstall_tensorflow()\nNote that you should also provide any options used in your original installation (e.g. method = \"conda\", version = \"gpu\", etc. )\ncuDNN 6.0\nTensorFlow v1.3 is built against version 6.0 of the cuDNN library from NVIDIA. Previous versions were built against cuDNN v5.1, so for installations running the GPU version of TensorFlow this means that you will need to install an updated version of cuDNN along with TensorFlow v1.3.\nUpdated installation instructions are available here: https://tensorflow.rstudio.com/tensorflow/installation_gpu.html.\nVersion 1.4 of TensorFlow is expected to migrate again to version 7.0 of cuDNN.\n\n\n",
    "preview": "posts/2017-08-17-tensorflow-v13-released/tensorflow-logo.png",
    "last_modified": "2021-06-29T19:29:07+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2015/1-choosing-personal-website/",
    "title": "Choosing a platform for a personal website",
    "description": "A general introduction and comparison of platforms for hosting a personal website. This is just a primer to the different options available and not a detailed and thorough comparison.",
    "author": [
      {
        "name": "rmf",
        "url": {}
      }
    ],
    "date": "2015-08-09",
    "categories": [
      "Web"
    ],
    "contents": "\n\nContents\nIntroduction\nDynamic Websites\nUnhosted websites\nFree hosted websites\nShared hosted websites\nVirtual Private Server\nDedicated server\nCloud computing\n\nStatic Websites\nConclusion\n\nIntroduction\nHello Internet! I finally have a website. I’ve been thinking about having a website for several years now but never actually got around to making one. It was mostly due to lack of focussed time rather than lack of ideas. I probably won’t have and don’t plan on having regular popular content. So my idea was to have a personal website showcasing my work, a bit about myself, contact, various social links and an occasional blog post. I spent quite a bit of time researching on potential set-up for the website from various hosting services to themes and templates. I thought I would go through some of the web hosting options out there especially for a personal website.\nDynamic Websites\nUnhosted websites\n\nIf you do not want to bother with web hosting and management, the best choice is to go for a system that is ready and managed. These come as restricted free services or full-fledged paid services. Drag and drop builders are provided. No coding or server management is required. One of the most popular and beautiful looking options is Squarespace. Others include Wix, Virb, Weebly, Strikingly and lots more. Another good example is wordpress.com for example, which is heavily restricted in terms of domain name, theme modification etc. Some other options in similar category with limited customisation include Blogger and Google sites. Google sites is probably from the 1990s and still feels and looks the same. I won’t include things like Facebook or Tumblr since I don’t see them as personal websites, but rather social media.\nFree hosted websites\n\nThere are several options for free hosted websites where you just have to create a user account and put in some details and you are set to go. These sort of web services have a range of features from custom domain names to site building tools. The best feature is of course that they are free. They might also provide free domain names like yoursite.provider.com. Most of them offer support for php, mysql databases etc with a graphical user interface without diving into code. The drawbacks with such services are that they are almost always shared which affects performance. The storage space and traffic bandwidth is limited. But, for a small personal website, this really shouldn’t be a problem. Some of such providers include 000webshost, awardspace, freewebhostingarea etc. Here is a small list on prchecker.\n\nShared hosted websites\nMost basic paid web hosting are so called ‘shared hosting’. The previously mentioned providers and almost all free providers also provide this sort of paid hosting. Free domain names are provided and the possibility of a custom domain is also possible. Storage and traffic bandwidth is increased. A wide range of pricing to fit your needs makes this an attractive option. Nevertheless, your website will still be shared on a physical location which means the cores and memory are shared between all other users/websites on the same computer. This might cause a drop in performance especially if another demanding website runs on the same computer. You also do not have root access to system administration. But, this option should suffice for most general users, private bloggers, small scale websites etc. PCMag has a nice list of such providers for 2015.\nVirtual Private Server\nThese are the upgraded version of shared hosting. You have your own private area that you have full control over as if you own it. You can configure anything any way you like. Handles traffic better. You need to load your own LAMP stack (Linux, Apache, MySQL, PHP or Perl) and set up your own server management. This means more technical work and demanding system administration and self regulated security. If done properly, has more security than shared hosting. You can install your own resource-hungry softwares. There are lots of resources on the web to compare differences between shared and VPS hosting. All major web hosting providers such as bluehost, hostgator, dreamhost etc. have the VPS option.\nDedicated server\nA dedicated server means that you are not getting a slice of the server, instead you get the whole server to your self. You can choose the cores and RAM and storage etc. System management and administration is demanding and usually command line. Some services may provide control panels and auto-installer scripts for commonly used applications. You can install any server OS, any web-server from Apache to Nginx. While there is effectively no sharing on computing resources, there might be bandwidth sharing. PCMag has yet another list for dedicated servers.\nCloud computing\ncloud computing servicesCloud computing is the newest trend. Cloud computing provides users with something called instances. These are virtual machines with any configuration as you would like. Any number of cores, whatever RAM, storage etc. Instances can be created and installed with any OS on the fly in a couple of minutes. You can install, configure, management anything anyway you like. Billing is usually time based. Hourly billing is usually used for intensive high-computing instances while a monthly billing is used for lower configurations. Websites implemented through cloud computing have their content on multiple servers at multiple locations at the same time which means that they less prone to downtime if a single physical server crashes somewhere. This may not always be the case depending on the set-up. There are countless articles debating the pros and cons of cloud computing. As a bottom line, cloud computing might be overkill for hosting a small website. The most prominent players are Amazon EC2 and Digital Ocean. Others include Vultr, Linode, Google Cloud and Microsoft Azure. Amazon AWS has a free tier option for new customers for the first one year.\nStatic Websites\nstatic website servicesStatic websites contain fixed static content. They do not use server-side scripting like PHP or databases like SQL. They are simple and easy to develop and maintain. They are used for small websites where content do not change very often. Static websites use only HTML, javascript and CSS. They do not have any fancy features or functionalities. They are the cheapest to host as they do not need any sort of live server. They can be hosted off Github or even Dropbox. Beautiful looking web pages can be created and updated simply using Markdown. Some of the popular platforms for generating static sites include Jekyll, Hugo, GitBook etc. Here is Dean Attali’s website which is a good example of a static website.\nConclusion\nUltimately, after all that brain racking, I decided (for now) to go with the cloud computing option of Vultr. I am running a wordpress site. I decided to go for the cloud computing option since I also wanted a live server to run an R shiny server for R shiny applications. Or else, I would’ve opted the static website option on Github pages.\n\n\n\n",
    "preview": "posts/2015/1-choosing-personal-website/featured.jpg",
    "last_modified": "2021-06-29T19:29:07+00:00",
    "input_file": {}
  }
]
